
# ğŸ“š Retrieval-Augmented Generation (RAG) with LLM

This project demonstrates how to implement **RAG (Retrieval-Augmented Generation)** using **Gradio** for the user interface and **Ollama's local LLMs (like Mistral)** for generating smart responses based on uploaded PDF documents.

---

## ğŸš€ Features

- ğŸ“„ Upload any PDF document
- ğŸ” Retrieve relevant content using RAG
- ğŸ¤– Generate LLM-based answers to user queries
- ğŸ§  Uses `ollama` and a locally run model like `mistral`
- ğŸŒ Clean, simple Gradio interface

---

## ğŸ› ï¸ Requirements

```bash
pip install gradio langchain pandas faiss-cpu ollama
