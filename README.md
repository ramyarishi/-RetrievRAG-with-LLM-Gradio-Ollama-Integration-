
# 📚 Retrieval-Augmented Generation (RAG) with LLM

This project demonstrates how to implement **RAG (Retrieval-Augmented Generation)** using **Gradio** for the user interface and **Ollama's local LLMs (like Mistral)** for generating smart responses based on uploaded PDF documents.

---

## 🚀 Features

- 📄 Upload any PDF document
- 🔍 Retrieve relevant content using RAG
- 🤖 Generate LLM-based answers to user queries
- 🧠 Uses `ollama` and a locally run model like `mistral`
- 🌐 Clean, simple Gradio interface

---

## 🛠️ Requirements

```bash
pip install gradio langchain pandas faiss-cpu ollama
